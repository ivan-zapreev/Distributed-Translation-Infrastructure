[Server Options]
    #The port to listen to; <unsigned integer>
    server_port=9001
    
    #The number of threads to run for sentence translation
    num_threads=25

    #The source language name in English, starting with capital letter; <string>
    source_lang=<source language>

    #The target language name in English, starting with capital letter; <string>
    target_lang=<target language>

[Language Models]
    #The language model file name (*.lm file extension); <string>
    lm_conn_string=<lm model file name>

    #The language model unknown word probability in the log_e space
    unk_word_log_e_prob=-10.7763748168945

    #The language model weight(s) used for tuning, are | separated
    lm_feature_weights=0.2

[Translation Models]
    #The translation model file name (*.tm file extension); <string>
    tm_conn_string=<tm model file name>

    #The translation model weight(s) used for tuning.
    #There should be as many weights as there is features in
    #the translation models, with the same order, are | separated.
    #The meaning of the weights is as follows:
    #   weight[0] is for p(f|e);
    #   weight[1] is for lex(p(f|e));
    #   weight[2] is for p(e|f);
    #   weight[3] is for lex(p(e|f));
    #   weight[4] is for phrase_penalty;
    tm_feature_weights=1.0|1.0|1.0|1.0|1.0

    #Stores the unknown entry features, these should be
    #as is and will be put to log scale and multiplied
    #with tm_feature_weights by the tool, are | separated
    tm_unk_features=4.539992e-5|4.539992e-5|4.539992e-5|4.539992e-5|2.71828182846

    #Only consider the top N translations for each phrase only; <unsigned integer>
    #If the value is set to <= 2 then there is no limit.
    tm_trans_lim=30

    #Only consider the translations with minimum probability p(f|e) and p(e|f)
    #larger than this value; <unsigned float>, without using feature weights
    tm_min_trans_prob=1e-20

    #Word penalty lambda is a value given for each produced word; <float>:
    #     size(target_phrase_words) * word_penalty
    # < 0.0  we prefer longer translations (more words in the target sentence)
    # == 0.0 there is no word penalty
    # > 0.0  we prefer shorter translations (less words in the target sentence)
    tm_word_penalty=-0.3

[Reordering Models]
    #The reordering model file name (*.rm file extension); <string>
    rm_conn_string=<rm model file name>

    #The reordering model weight(s) used for tuning
    #There should be as many weights as there is features
    #in the translation models, with the same order,
    #are | separated
    rm_feature_weights=1.0|1.0|1.0|1.0|1.0|1.0

[Decoding Options]
    #The pruning threshold is to be a <unsigned float> it is 
    #the %/100 deviation from the best hypothesis score.
    de_pruning_threshold=0.1

    #The stack capacity for stack pruning; <unsigned integer>
    de_stack_capacity=100

    #Stores the maximum considered source phrase length; <unsigned integer>
    de_max_source_phrase_length=7

    #Stores the maximum considered target phrase length,
    #should agree with the language and translation
    #servers; <unsigned integer>
    de_max_target_phrase_length=7

    #The distortion limit to use; <integer>
    #The the number of words from the last non-translated
    #word to consider in hypothesis expansion.
    #A negative value means no distortion limit.
    de_dist_lim=5

    #The lambda parameter to be used as a multiplier for the
    #linear distortion: <float>, can be positive or negative
    # > 0.0  is a linear distortion penalty
    # == 0.0 the linear distortion is not taken into account
    # < 0.0 is a linear distortion reward
    de_lin_dist_penalty=1.0

    #The the tuning search lattice generation flag; <bool>:
    #This flag only works if the server is compiled with
    #the IS_SERVER_TUNING_MODE macro flag to true, otherwise
    #it is ignored, i.e. is internally re-set to false.
    de_is_gen_lattice=false

    #Stores the lattice data folder location for where the
    #generated lattice information is to be stored.
    de_lattices_folder=./lattices
    
    #The file name extention for the feature id to name mapping file
    #needed for tuning. The file will be generated if the lattice
    #generation is enabled. It will have the same name as the config
    #file plus this extention.
    de_lattice_id2name_file_ext=feature_id2name

    #The file name extention for the feature scores file needed for tuning.
    #The file will be generated if the lattice generation is enabled.
    #It will have the same name as the session id plus the translation
    #job id plus this extention.
    de_feature_scores_file_ext=feature_scores

    #The file name extention for the lattice file needed for tuning.
    #The file will be generated if the lattice generation is enabled.
    #It will have the same name as the session id plus the translation
    #job id plus this extention.
    de_lattice_file_ext=lattices
